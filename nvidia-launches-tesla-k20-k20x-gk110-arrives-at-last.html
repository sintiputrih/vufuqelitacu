<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>GK110: The GPU Behind Tesla K20 -</title><meta name=robots content="index,follow,noarchive"><meta name=description content="GK110: The GPU Behind Tesla K20
Now that we’ve discussed the Telsa K20 series from the big-picture perspective of performance, configurations, pricing, and the marketplace, we can finally dive into the technical underpinnings of the K20.
Announced alongside the Tesla K20 back at NVIDIA’s GTC 2012 was the GPU that would be powering it: GK110. In a reversal of their usual pattern, GK110 was to be NVIDIA’s first compute-oriented Kepler GPU (GK10X having been significantly stripped for gaming efficiency purposes), but it would be the last Kepler GPU to launch."><meta name=author content="Martina Birk"><link rel="preload stylesheet" as=style href=https://assets.cdnweb.info/hugo/paper/css/app.css><link rel="preload stylesheet" as=style href=https://assets.cdnweb.info/hugo/paper/css/an-old-hope.min.css><script defer src=https://assets.cdnweb.info/hugo/paper/js/highlight.min.js onload=hljs.initHighlightingOnLoad()></script>
<link rel=preload as=image href=./theme.png><link rel=icon href=./favicon.ico><link rel=apple-touch-icon href=./apple-touch-icon.png><meta name=generator content="Hugo 0.98.0"><meta property="og:title" content="GK110: The GPU Behind Tesla K20"><meta property="og:description" content="GK110: The GPU Behind Tesla K20 Now that weve discussed the Telsa K20 series from the big-picture perspective of performance, configurations, pricing, and the marketplace, we can finally dive into the technical underpinnings of the K20."><meta property="og:type" content="article"><meta property="og:url" content="/nvidia-launches-tesla-k20-k20x-gk110-arrives-at-last.html"><meta property="article:section" content="post"><meta property="article:published_time" content="2024-03-08T00:00:00+00:00"><meta property="article:modified_time" content="2024-03-08T00:00:00+00:00"><meta itemprop=name content="GK110: The GPU Behind Tesla K20"><meta itemprop=description content="GK110: The GPU Behind Tesla K20 Now that weve discussed the Telsa K20 series from the big-picture perspective of performance, configurations, pricing, and the marketplace, we can finally dive into the technical underpinnings of the K20."><meta itemprop=datePublished content="2024-03-08T00:00:00+00:00"><meta itemprop=dateModified content="2024-03-08T00:00:00+00:00"><meta itemprop=wordCount content="1467"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="GK110: The GPU Behind Tesla K20"><meta name=twitter:description content="GK110: The GPU Behind Tesla K20 Now that weve discussed the Telsa K20 series from the big-picture perspective of performance, configurations, pricing, and the marketplace, we can finally dive into the technical underpinnings of the K20."></head><body class=not-ready data-menu=true><header class=header><p class=logo><a class=site-name href=./index.html>ZBlogR</a><a class=btn-dark></a></p><script>let bodyClx=document.body.classList,btnDark=document.querySelector(".btn-dark"),sysDark=window.matchMedia("(prefers-color-scheme: dark)"),darkVal=localStorage.getItem("dark"),setDark=e=>{bodyClx[e?"add":"remove"]("dark"),localStorage.setItem("dark",e?"yes":"no")};setDark(darkVal?darkVal==="yes":sysDark.matches),requestAnimationFrame(()=>bodyClx.remove("not-ready")),btnDark.addEventListener("click",()=>setDark(!bodyClx.contains("dark"))),sysDark.addEventListener("change",e=>setDark(e.matches))</script><nav class=menu><a href=./sitemap.xml>Sitemap</a></nav></header><main class=main><article class=post-single><header class=post-title><p><time>Mar 8, 2024</time>
<span>Martina Birk</span></p><h1>GK110: The GPU Behind Tesla K20</h1></header><section class=post-content><p><strong>GK110: The GPU Behind Tesla K20</strong></p><p>Now that we’ve discussed the Telsa K20 series from the big-picture perspective of performance, configurations, pricing, and the marketplace, we can finally dive into the technical underpinnings of the K20.</p><p>Announced alongside the Tesla K20 back at NVIDIA’s GTC 2012 was the GPU that would be powering it: GK110. In a reversal of their usual pattern, GK110 was to be NVIDIA’s first compute-oriented Kepler GPU (GK10X having been significantly stripped for gaming efficiency purposes), but it would be the last Kepler GPU to launch. Whereas in the Fermi generation we saw GF100 first and could draw some conclusions about the eventual Tesla cards from that, GK110 has been a true blank slate. On the other hand because it builds upon NVIDIA’s earlier Kepler GPUs, we can draw a clear progression from GK104 to GK110.</p><p align=center><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/6446/gk110die-2.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></p><p>GK110 is NVIDIA’s obligatory big-die GPU. We don’t have a specific die size, but at 7.1 billion transistors it is now the biggest GPU ever built in terms of transistors, dwarfing the 3.5B transistor GK104 and the 4.3B transistor Tahiti GPU from AMD. These big-die GPUs are unwieldy from a fabrication and power consumption perspective, but the end result is that the performance per GPU is unrivaled due to the fact that so many tasks (both graphical and compute) are embarrassingly parallel and map well to the large arrays of streaming processors found in a GPU.</p><p>Like GF100 before it, GK110 has been built to fill multiple roles. For today’s launch we’re mostly talking about it from a compute perspective – and indeed most of the die is tied up compute hardware – but it also has all of the graphics hardware we would expect in an NVIDIA GPU. Altogether it packs 15 SMXes and 6 ROP/L2/memory&nbsp; controller blocks, versus 8 SMXes and 4 ROP/L2/memory blocks on GK104. Not accounting for clockspeeds this gives GK110 87% more compute performance and 50% more memory bandwidth than GK104. But there’s a great deal more to GK110 than just a much larger collection of functional units.</p><table align=center border=0 cellpadding=0 cellspacing=1 width=575><tbody><tr class=tgrey><td align=center colspan=7>NVIDIA GPU Comparison</td></tr><tr class=tlblue><td width=132>&nbsp;</td><td align=center valign=middle width=110>Fermi GF100</td><td align=center valign=middle width=110>Fermi GF104</td><td align=center valign=middle width=108>Kepler GK104</td><td align=center valign=middle width=109>Kepler GK110</td></tr><tr><td class=tlgrey>Compute Capability</td><td align=center valign=middle>2.0</td><td align=center valign=middle>2.1</td><td align=center valign=middle>3.0</td><td align=center valign=middle>3.5</td></tr><tr><td class=tlgrey>Threads/Warp</td><td align=center valign=middle>32</td><td align=center valign=middle>32</td><td align=center valign=middle>32</td><td align=center valign=middle>32</td></tr><tr><td class=tlgrey>Max Warps/SM(X)</td><td align=center valign=middle>48</td><td align=center valign=middle>48</td><td align=center valign=middle>64</td><td align=center valign=middle>64</td></tr><tr><td class=tlgrey>Max Threads/SM(X)</td><td align=center valign=middle>1536</td><td align=center valign=middle>1536</td><td align=center valign=middle>2048</td><td align=center valign=middle>2048</td></tr><tr><td class=tlgrey>Register File</td><td align=center valign=middle>32,768</td><td align=center valign=middle>32,768</td><td align=center valign=middle>65,536</td><td align=center valign=middle>65,536</td></tr><tr><td class=tlgrey>Max Registers/Thread</td><td align=center valign=middle>63</td><td align=center valign=middle>63</td><td align=center valign=middle>63</td><td align=center valign=middle>255</td></tr><tr><td class=tlgrey>Shared Mem Config</td><td align=center valign=middle>16K<br>48K</td><td align=center valign=middle>16K<br>48K</td><td align=center valign=middle>16K<br>32K<br>48K</td><td align=center valign=middle>16K<br>32K<br>48K</td></tr><tr><td class=tlgrey>Hyper-Q</td><td align=center valign=middle>No</td><td align=center valign=middle>No</td><td align=center valign=middle>No</td><td align=center valign=middle>Yes</td></tr><tr><td class=tlgrey>Dynamic Parallelism</td><td align=center valign=middle>No</td><td align=center valign=middle>No</td><td align=center valign=middle>No</td><td align=center valign=middle>Yes</td></tr></tbody></table><p>Fundamentally GK110 is a highly enhanced if not equally specialized version of the Kepler architecture. The SMX, first introduced with GK104, is the basis of GK110. Each GK104 SMX contained 192 FP32 CUDA cores, 8 FP64 CUDA cores, 256KB of register file space, 64KB of L1 cache, 48KB of uniform cache. In turn it was fed by 4 warp schedulers, each with two dispatch units, allowing GK104 to issue instructions from warps in a superscalar manner.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/5840/GK110Block_575px.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p align=center><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/5840/GK110SMX.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto><br>GK110 SMX</p><p>GK110 builds on that by keeping the same general design, but tweaking it for GK110’s compute-focused needs. The single biggest change here is that rather than 8 FP64 CUDA cores GK110 has 64 FP64 CUDA cores, giving it 8 times the FP64 performance of a GK104 SMX. The SMXes are otherwise very similar at a high level, featuring the same 256KB of register file space, 64KB of L1 cache, 48KB of uniform cache, and the same warp scheduler structure. This of course does not include a number of low level changes that further set apart GK104 and GK110.</p><p>Meanwhile this comparison gets much more jarring if we take a look at GK110 versus GF100 and by extension Tesla K20 versus its direct predecessors, the Fermi based Tesla family. The GK110 SMX compared to the GF100 SM is nothing short of a massive change. Superficially NVIDIA has packed many more CUDA cores into an SMX than they have an SM due to the change from a shader design that ran fewer CUDA cores at a very high (double pumped) clockspeed to a design that runs many more CUDA cores at a lower (single pumped) clockspeed, but they also have changed their warp execution model on its head in the process.</p><p align=center><img src=https://cdn.statically.io/img/images.anandtech.com/reviews/video/NVIDIA/GF100/sm.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto><br>GF100/GF110 SM</p><p>GF100 was essentially a thread level parallelism design, with each SM executing a single instruction from up to two warps. At the same time certain math instructions had variable latencies, so GF100 utilized a complex hardware scoreboard to do the necessary scheduling. Compared to that, GK110 introduces instruction level parallelism to the mix, making the GPU reliant on a mix of high TLP and high ILP to achieve maximum performance. The GPU now executes from 4 warps, ultimately executing up to 8 instructions at once if all of the warps have ILP-suitable instructions waiting. At the same time scheduling has been moved from hardware to software, with NVIDIA’s compiler now statically scheduling warps thanks to the fact that every math instruction now has a fixed latency. Finally, to further improve SMX utilization FP64 instructions can now be paired with other instructions, whereas on GF100 they had to be done on their own.</p><p>The end result is that at an execution level NVIDIA has sacrificed some of GF100’s performance consistency by introducing superscalar execution – and ultimately becoming reliant on it for maximum performance. At the same time they have introduced a new type of consistency (and removed a level of complexity) by moving to fixed latency instructions and a static scheduled compiler. Thankfully a ton of these details are abstracted from programmers and handled by NVIDIA’s compiler, but for HPC users who are used to getting their hands dirty with low level code they are going to find that GK110 is more different than it would seem at first glance.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/5699/Scheduler_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p>With that said, even with the significant changes to their warp execution model, GK110 brings more changes yet. We can’t hope to replicate the sheer amount of depth <a href=#>NVIDIA’s own GK110 whitepaper covers</a>, but there are several other low-level changes that further separate GK110 from GF100.</p><p>Space and bandwidth for both the register file and the L2 cache have been greatly increased for GK110. At the SMX level GK110 has 256KB of register file space, composed of 65K 32bit registers, as compared to 128KB of such space (32K registers) on GF100. Bandwidth to those register files has in turn been doubled, allowing GK110 to read from those register files faster than ever before. As for the L2 cache, it has received a very similar treatment. GK110 uses an L2 cache up to 1.5MB, twice as big as GF110; and that L2 cache bandwidth has also been doubled.</p><p align=center><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/6446/KeplerMem.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></p><p>What makes this all the more interesting is that while NVIDIA significantly increased the number of CUDA cores in an SM(X), in fact by far more than the increase in cache and register file sizes, they only marginally increased the number of threads that are actually active on an SMX. Each GK110 SMX can only have up to 2K threads at any time, 1.33x that of GF100 and its 1.5K threads. So as a result GK110 is working from a thread pool only slightly larger than what GF100 worked with, which means that despite the increase in CUDA cores they actually improve their performance in register-starved scenarios as there are more registers available to each thread. This goes hand in hand with an increase in the total number of registers each thread can address, moving from 63 registers per thread on GF100 to 255 registers per thread with GK110.</p><p>While we’re on the subject of caches, it’s also worth noting that NVIDIA has reworked their texture cache to be more useful for compute. On GF100 the 12KB texture cache was just that, a texture cache, only available to the texture units. As it turns out, clever programmers were using the texture cache as another data cache by mapping normal data at texture data, so NVIDIA has promoted the texture cache to a larger, more capable cache on GK110. Now measuring 48KB in size, in compute mode the texture cache becomes a read-only cache, specializing in unaligned memory access patterns. Furthermore error detection capabilities have been added to it to make it safer for use with workloads that rely on ECC.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/6446/Shuffle_575px.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p>Last, but certainly not least in our low level look, NVIDIA has added a number of new instructions and operations to GK110 to further improve performance. New shuffle instructions allow for threads within a warp to share (i.e. shuffle) data without going to shared memory, making the process much faster than the old load/share/store method. Meanwhile atomic operations have also been overhauled, with NVIDIA both speeding up the execution speed of atomic operations and adding some FP64 operations that were previously only available for FP32 data.</p><p class=postsid style=color:rgba(255,0,0,0)>ncG1vNJzZmivp6x7orrAp5utnZOde6S7zGiqoaenZIN1gJVopa%2BhlJ6ubrjArqWcoJWoerWx0qWYZqNiZXqsfo%2BxZKCjYWZ9bq3Rq6CvnaNirrV5y5qqrWdj</p></section><nav class=post-nav><a class=prev href=./william-spaulding-obituary.html><span>←</span><span>William Spaulding Obituary, Accidental Death In A Car in Newton, Massachusetts, A Sad Loss!</span></a>
<a class=next href=./ian-alexander-sr-net-worth-2024.html><span>Ian Alexander Sr. Net Worth 2024</span><span>→</span></a></nav></article></main><footer class=footer><p>&copy; 2024 <a href=./></a></p><p>Powered by <a href=https://gohugo.io/ rel=noopener target=_blank>Hugo️️</a>️</p></footer><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/banner.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/tracking_server_6.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script>var _paq=window._paq=window._paq||[];_paq.push(["trackPageView"]),_paq.push(["enableLinkTracking"]),function(){e="//analytics.cdnweb.info/",_paq.push(["setTrackerUrl",e+"matomo.php"]),_paq.push(["setSiteId","1"]);var e,n=document,t=n.createElement("script"),s=n.getElementsByTagName("script")[0];t.async=!0,t.src=e+"matomo.js",s.parentNode.insertBefore(t,s)}()</script></body></html>